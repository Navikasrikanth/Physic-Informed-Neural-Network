{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navikasrikanth/Physic-Informed-Neural-Network/blob/main/PINN_Model_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6IjSpv2bQHE"
      },
      "source": [
        "#Step 1: Define Poisson’s Equation and Boundary Conditions\n",
        "##We are solving the 2D Poisson equation:\n",
        "\n",
        "Δu(x, y) = f(x, y), for (x, y) ∈ [0,1] × [0,1]\n",
        "\n",
        "Boundary conditions: u(x, y) = 0, for (x, y) on the boundary (Dirichlet BC).\n",
        "\n",
        "Here, Δ is the Laplace operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M_LruOfa8Yp",
        "outputId": "e8815546-2aba-4c61-86a8-0909e553235d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture: Fully-connected MLP (2 -> 200 -> 200 -> 200 -> 1)\n",
            "Solution type: sin(pi*x)*sin(pi*y)\n",
            "Activation: Tanh\n",
            "Learning rate: 0.0005\n",
            "Noise level: 0.0\n",
            "Collocation points (interior): 30000\n",
            "Boundary points: 800\n",
            "Epochs (forward): 3000 Epochs (inverse): 5000\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "torch.set_default_dtype(torch.float32)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "\n",
        "POLY_TYPE = \"sin(pi*x)*sin(pi*y)\"\n",
        "ACTIVATION = \"Tanh\"\n",
        "LEARNING_RATE = 5e-4\n",
        "NOISE_LEVEL = 0.0\n",
        "N_F = 30000\n",
        "N_B = 800\n",
        "EPOCHS_FORWARD = 3000\n",
        "EPOCHS_INVERSE = 5000\n",
        "PRINT_EVERY = 100\n",
        "\n",
        "print(\"Architecture: Fully-connected MLP (2 -> 200 -> 200 -> 200 -> 1)\")\n",
        "print(\"Solution type:\", POLY_TYPE)\n",
        "print(\"Activation:\", ACTIVATION)\n",
        "print(\"Learning rate:\", LEARNING_RATE)\n",
        "print(\"Noise level:\", NOISE_LEVEL)\n",
        "print(\"Collocation points (interior):\", N_F)\n",
        "print(\"Boundary points:\", N_B)\n",
        "print(\"Epochs (forward):\", EPOCHS_FORWARD, \"Epochs (inverse):\", EPOCHS_INVERSE)\n",
        "print(\"Device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9h6_s0nbjCQ"
      },
      "source": [
        "###Generate Forward Problem Data\n",
        "\n",
        "In this step, we set up the data for the forward problem:\n",
        "\n",
        "- We define the exact solution `u(x, y)` as `A * sin(pi*x) * sin(pi*y)`.\n",
        "- The source function `f(x, y)` is defined as `-2 * pi^2 * A * sin(pi*x) * sin(pi*y)`.\n",
        "- The boundary function `g(x, y)` gives the values of `u` on the edges of the domain.\n",
        "\n",
        "Next, we generate:\n",
        "\n",
        "- **Interior collocation points** randomly inside the square domain.\n",
        "- **Boundary points** along the edges of the domain.\n",
        "- **Boundary values** using the boundary function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq3KFjqsbXo_"
      },
      "outputs": [],
      "source": [
        "A_true = 1.0\n",
        "\n",
        "def u_exact_func(x, y, A=A_true):\n",
        "  return A * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
        "\n",
        "def f_forward_func(x, y, A=A_true):\n",
        "  return -2 * (np.pi**2) * A * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
        "\n",
        "def g_boundary(x, y, A=A_true):\n",
        "  return u_exact_func(x, y, A=A)\n",
        "\n",
        "x_f = torch.rand(N_F, 1, device=device) #compute collocation points\n",
        "y_f = torch.rand(N_F, 1, device=device)\n",
        "\n",
        "n_edge = N_B // 4\n",
        "x_b = torch.cat([                #combine all boundary points\n",
        "torch.rand(n_edge,1),\n",
        "torch.zeros(n_edge,1),\n",
        "torch.ones(n_edge,1),\n",
        "torch.rand(n_edge,1)\n",
        "], dim=0).to(device)\n",
        "y_b = torch.cat([\n",
        "torch.zeros(n_edge,1),\n",
        "torch.rand(n_edge,1),\n",
        "torch.rand(n_edge,1),\n",
        "torch.ones(n_edge,1)\n",
        "], dim=0).to(device)\n",
        "\n",
        "u_b = g_boundary(x_b, y_b).to(device)        #assign boundary points to the boumdary function and predict exact boundary values\n",
        "if NOISE_LEVEL > 0:\n",
        "  u_b = u_b + NOISE_LEVEL * torch.randn_like(u_b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idLod6K0cj_y"
      },
      "source": [
        "# Define Neural Network\n",
        "\n",
        "In this step, we define a simple fully connected neural network with:\n",
        "\n",
        "- Input: two values (x and y)\n",
        "- Three hidden layers with 200 neurons and Tanh activation\n",
        "- Output: single value u(x, y)\n",
        "- Xavier initialization for all linear layer weights and zero initialization for biases\n",
        "\n",
        "\n",
        "## **Xavier (Glorot) Initialization**\n",
        "Xavier initialization (also called Glorot initialization) is a way to set the initial weights of a neural network so that signals flow properly through the layers at the start of training.\n",
        "\n",
        "\n",
        "When training neural networks, how we initialize weights matters.  \n",
        "- If weights are **too small** → activations and gradients vanish.  \n",
        "- If weights are **too large** → activations and gradients explode.  \n",
        "\n",
        "\n",
        "**Xavier initialization** (Glorot & Bengio, 2010) balances this by keeping the variance of activations roughly the same across all layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz-f_iLZbosw"
      },
      "outputs": [],
      "source": [
        "class PINN(nn.Module):                                       #initialise a fully connected nn with x,y inputs\n",
        "  def __init__(self, hidden_dim=200, activation=nn.Tanh):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(2, hidden_dim)\n",
        "    self.act1 = activation()\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.act2 = activation()\n",
        "    self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.act3 = activation()\n",
        "    self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    # Xavier init\n",
        "    for m in [self.fc1, self.fc2, self.fc3, self.fc4]:\n",
        "      nn.init.xavier_normal_(m.weight)\n",
        "      nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x, y):                  #moving forward by using activation function through each layer till output\n",
        "    inp = torch.cat([x, y], dim=1)\n",
        "    z = self.act1(self.fc1(inp))\n",
        "    z = self.act2(self.fc2(z))\n",
        "    z = self.act3(self.fc3(z))\n",
        "    out = self.fc4(z)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCo5qYZddIWz"
      },
      "source": [
        "# Define Loss Function\n",
        "\n",
        "In this step, we define how the model will be trained:\n",
        "\n",
        "- Compute the Laplacian (second derivatives) of the predicted u(x, y) using automatic differentiation.\n",
        "- Compute the PDE residual loss at interior collocation points.\n",
        "- Compute the boundary loss at boundary points.\n",
        "- Total loss = PDE residual loss + boundary loss\n",
        "- Return both the total loss and individual losses for monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl0WWjvVdJiC"
      },
      "outputs": [],
      "source": [
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "def laplacian_u(model, x, y):                             #computes uxx+uyy using automatic differentiation\n",
        "  x_req = x.clone().detach().requires_grad_(True)\n",
        "  y_req = y.clone().detach().requires_grad_(True)\n",
        "  u = model(x_req, y_req)\n",
        "  grad_u = torch.autograd.grad(u, [x_req, y_req], grad_outputs=torch.ones_like(u), create_graph=True)\n",
        "  u_x = grad_u[0]\n",
        "  u_y = grad_u[1]\n",
        "  u_xx = torch.autograd.grad(u_x, x_req, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
        "  u_yy = torch.autograd.grad(u_y, y_req, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
        "  return u_xx + u_yy\n",
        "\n",
        "def pinn_loss_forward(model, x_f, y_f, x_b, y_b, u_b):\n",
        "  f_pred = laplacian_u(model, x_f, y_f)           #computing left hand side u(x,y) at collocation points\n",
        "  f_true = f_forward_func(x_f, y_f).to(device)    #computing right hand side true function f(x,y)\n",
        "  loss_f = mse_loss(f_pred, f_true)               #residual loss\n",
        "  u_b_pred = model(x_b, y_b)\n",
        "  loss_b = mse_loss(u_b_pred, u_b)                #boundary loss\n",
        "  return loss_f + loss_b, loss_f.item(), loss_b.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb8uRCA9dPT7"
      },
      "source": [
        "# Train the Forward PINN\n",
        "\n",
        "In this step, we train the neural network to solve the Poisson equation:\n",
        "\n",
        "- Use the Adam optimizer with a specified learning rate.\n",
        "- For each epoch:\n",
        "  - Compute the total loss using the `pinn_loss_forward` function.\n",
        "  - Perform backpropagation and update the model parameters.\n",
        "  - Periodically print the total loss, PDE residual loss, and boundary loss to monitor training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C8gDKcLdj6m",
        "outputId": "794ddf46-a41b-4ba7-e149-60a49b5b4666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting forward training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Forward] Epoch 1/3000  Total: 1.122806e+00  f_loss: 1.010256e+00  b_loss: 1.125498e-01\n",
            "[Forward] Epoch 100/3000  Total: 1.165063e-01  f_loss: 8.257391e-02  b_loss: 3.393244e-02\n",
            "[Forward] Epoch 200/3000  Total: 4.352804e-02  f_loss: 3.015504e-02  b_loss: 1.337300e-02\n",
            "[Forward] Epoch 300/3000  Total: 1.119252e-02  f_loss: 5.946984e-03  b_loss: 5.245537e-03\n",
            "[Forward] Epoch 400/3000  Total: 2.564748e-03  f_loss: 1.413503e-03  b_loss: 1.151245e-03\n",
            "[Forward] Epoch 500/3000  Total: 7.420559e-04  f_loss: 4.886449e-04  b_loss: 2.534110e-04\n",
            "[Forward] Epoch 600/3000  Total: 3.618877e-04  f_loss: 2.715364e-04  b_loss: 9.035126e-05\n",
            "[Forward] Epoch 700/3000  Total: 1.301337e-03  f_loss: 2.559986e-04  b_loss: 1.045338e-03\n",
            "[Forward] Epoch 800/3000  Total: 1.632934e-04  f_loss: 1.351482e-04  b_loss: 2.814514e-05\n",
            "[Forward] Epoch 900/3000  Total: 2.947232e-04  f_loss: 1.196877e-04  b_loss: 1.750356e-04\n",
            "[Forward] Epoch 1000/3000  Total: 1.091678e-04  f_loss: 9.049944e-05  b_loss: 1.866836e-05\n",
            "[Forward] Epoch 1100/3000  Total: 1.007759e-04  f_loss: 7.916103e-05  b_loss: 2.161491e-05\n",
            "[Forward] Epoch 1200/3000  Total: 3.409056e-04  f_loss: 1.045428e-04  b_loss: 2.363628e-04\n",
            "[Forward] Epoch 1300/3000  Total: 7.956504e-05  f_loss: 6.405161e-05  b_loss: 1.551343e-05\n",
            "[Forward] Epoch 1400/3000  Total: 6.805334e-05  f_loss: 5.810536e-05  b_loss: 9.947978e-06\n",
            "[Forward] Epoch 1500/3000  Total: 1.286236e-04  f_loss: 5.933163e-05  b_loss: 6.929197e-05\n",
            "[Forward] Epoch 1600/3000  Total: 5.400158e-05  f_loss: 4.865213e-05  b_loss: 5.349451e-06\n",
            "[Forward] Epoch 1700/3000  Total: 4.924173e-05  f_loss: 4.467227e-05  b_loss: 4.569460e-06\n",
            "[Forward] Epoch 1800/3000  Total: 4.850749e-05  f_loss: 4.231727e-05  b_loss: 6.190221e-06\n",
            "[Forward] Epoch 1900/3000  Total: 4.224408e-05  f_loss: 3.879925e-05  b_loss: 3.444829e-06\n",
            "[Forward] Epoch 2000/3000  Total: 5.521657e-05  f_loss: 3.754985e-05  b_loss: 1.766672e-05\n",
            "[Forward] Epoch 2100/3000  Total: 3.702184e-05  f_loss: 3.413041e-05  b_loss: 2.891428e-06\n",
            "[Forward] Epoch 2200/3000  Total: 3.958724e-05  f_loss: 3.306025e-05  b_loss: 6.526984e-06\n",
            "[Forward] Epoch 2300/3000  Total: 3.268659e-05  f_loss: 3.023409e-05  b_loss: 2.452497e-06\n",
            "[Forward] Epoch 2400/3000  Total: 8.740387e-05  f_loss: 3.971658e-05  b_loss: 4.768729e-05\n",
            "[Forward] Epoch 2500/3000  Total: 2.926876e-05  f_loss: 2.666770e-05  b_loss: 2.601055e-06\n",
            "[Forward] Epoch 2600/3000  Total: 2.768051e-05  f_loss: 2.538270e-05  b_loss: 2.297806e-06\n",
            "[Forward] Epoch 2700/3000  Total: 2.546481e-05  f_loss: 2.351794e-05  b_loss: 1.946870e-06\n",
            "[Forward] Epoch 2800/3000  Total: 2.593460e-05  f_loss: 2.286401e-05  b_loss: 3.070595e-06\n",
            "[Forward] Epoch 2900/3000  Total: 2.303069e-05  f_loss: 2.102595e-05  b_loss: 2.004744e-06\n",
            "[Forward] Epoch 3000/3000  Total: 2.147097e-05  f_loss: 1.965063e-05  b_loss: 1.820339e-06\n"
          ]
        }
      ],
      "source": [
        "# ---------- Forward training ----------\n",
        "model = PINN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "print(\"Starting forward training...\")\n",
        "for epoch in range(1, 3001):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    f_pred = laplacian_u(model, x_f, y_f)\n",
        "    f_true = f_forward_func(x_f, y_f).to(device)\n",
        "\n",
        "    # Normalize PDE residual\n",
        "    loss_f = torch.mean((f_pred - f_true)**2) / (torch.mean(f_true**2) + 1e-8)\n",
        "    u_b_pred = model(x_b, y_b)\n",
        "    loss_b = mse_loss(u_b_pred, u_b)\n",
        "\n",
        "    total_loss = loss_f + loss_b\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0 or epoch == 1:\n",
        "        print(f\"[Forward] Epoch {epoch}/3000  Total: {total_loss.item():.6e}  f_loss: {loss_f.item():.6e}  b_loss: {loss_b.item():.6e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_0uBjMnqHZO"
      },
      "source": [
        "#Inverse Problem Training\n",
        "\n",
        "Start from the forward-trained model to estimate the unknown 𝛼.\n",
        "\n",
        "Use beta as a trainable parameter with 𝛼=exp(𝛽)\n",
        "\n",
        "Train for 12,000 epochs with weighted PDE and boundary losses.\n",
        "\n",
        "Track the best\n",
        "𝛼 and total loss.\n",
        "\n",
        "Fine-tune for 500 additional epochs at a lower learning rate to refine\n",
        "𝛼\n",
        "\n",
        "Epoch display shows correct progress for both main training and fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJEXlAJYdxgv"
      },
      "outputs": [],
      "source": [
        "# ---------- Inverse training ----------\n",
        "beta = torch.tensor(np.log(0.8), requires_grad=True, device=device)  # reparam alpha=exp(beta)\n",
        "model_inv = PINN().to(device)\n",
        "model_inv.load_state_dict(model.state_dict())  # ✅ starts from forward-trained model\n",
        "\n",
        "optimizer_inv = optim.Adam([\n",
        "    {'params': model_inv.parameters(), 'lr': 1e-3},\n",
        "    {'params': [beta], 'lr': 3e-3}\n",
        "])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_inv, mode='min', factor=0.5, patience=300, min_lr=1e-6\n",
        ")\n",
        "\n",
        "def pinn_loss_inverse(model, beta_param, x_f, y_f, x_b, y_b, u_b):\n",
        "    alpha_param = torch.exp(beta_param)\n",
        "    f_pred = laplacian_u(model, x_f, y_f)\n",
        "    f_true = (-2 * (np.pi**2) * alpha_param * torch.sin(np.pi * x_f) * torch.sin(np.pi * y_f)).to(device)\n",
        "    loss_f = torch.mean((f_pred - f_true)**2) / (torch.mean(f_true**2) + 1e-8)\n",
        "    u_b_pred = model(x_b, y_b)\n",
        "    loss_b = mse_loss(u_b_pred, u_b)\n",
        "    total = 2.0 * loss_f + 1.0 * loss_b   # Increase PDE focus\n",
        "\n",
        "    return total, loss_f.item(), loss_b.item(), alpha_param.item()\n",
        "\n",
        "print(\"Starting inverse training...\")\n",
        "best_loss = 1e9\n",
        "best_alpha = None\n",
        "\n",
        "total_epochs = 12000  # main inverse training\n",
        "for epoch in range(1, total_epochs + 1):\n",
        "    model_inv.train()\n",
        "    optimizer_inv.zero_grad()\n",
        "\n",
        "    total_loss, loss_f_inv, loss_b_inv, alpha_val = pinn_loss_inverse(model_inv, beta, x_f, y_f, x_b, y_b, u_b)\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(model_inv.parameters()) + [beta], max_norm=2.0)\n",
        "    optimizer_inv.step()\n",
        "    scheduler.step(total_loss.detach())\n",
        "\n",
        "    if total_loss.item() < best_loss:\n",
        "        best_loss = total_loss.item()\n",
        "        best_alpha = alpha_val\n",
        "\n",
        "    if epoch % 100 == 0 or epoch == 1:\n",
        "        print(f\"[Inverse] Epoch {epoch}/{total_epochs}  Total: {total_loss.item():.6e}  f_loss: {loss_f_inv:.6e}  b_loss: {loss_b_inv:.6e}  alpha={alpha_val:.6f}\")\n",
        "\n",
        "learned_alpha = torch.exp(beta).item()\n",
        "print(\"\\nFinal learned alpha from inverse training:\", learned_alpha)\n",
        "print(\"Best alpha encountered during training:\", best_alpha)\n",
        "\n",
        "# Fine-tune last 500 epochs at lower LR\n",
        "for g in optimizer_inv.param_groups:\n",
        "    g['lr'] = 5e-4  # smaller LR\n",
        "\n",
        "fine_tune_start = 12001\n",
        "fine_tune_end = 12500\n",
        "\n",
        "for epoch in range(fine_tune_start, fine_tune_end + 1):\n",
        "    total_loss, loss_f_inv, loss_b_inv, alpha_val = pinn_loss_inverse(model_inv, beta, x_f, y_f, x_b, y_b, u_b)\n",
        "    optimizer_inv.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(model_inv.parameters()) + [beta], max_norm=2.0)\n",
        "    optimizer_inv.step()\n",
        "\n",
        "    # print relative to fine-tune epochs\n",
        "    if (epoch - fine_tune_start + 1) % 50 == 0 or epoch == fine_tune_start:\n",
        "        ft_epoch = epoch - fine_tune_start + 1\n",
        "        print(f\"[Inverse Fine-tune] Epoch {ft_epoch}/500  Total: {total_loss.item():.6e}  f_loss: {loss_f_inv:.6e}  b_loss: {loss_b_inv:.6e}  alpha={alpha_val:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xxkJV3eJNM"
      },
      "source": [
        "###Predict Solutions for Forward and Inverse Problems\n",
        "\n",
        "In this step, we:\n",
        "\n",
        "- Generate a grid of points in the domain for testing.\n",
        "- Use the trained **forward model** to predict u(x, y) across the domain.\n",
        "- Use the trained **inverse model** to predict u(x, y) and recover the learned parameter alpha.\n",
        "- Store the exact solution for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8rZdusdd1X8"
      },
      "outputs": [],
      "source": [
        "n_test = 50\n",
        "x_vals = torch.linspace(0,1,n_test, device=device).reshape(-1,1)\n",
        "y_vals = torch.linspace(0,1,n_test, device=device).reshape(-1,1)\n",
        "Xg, Yg = torch.meshgrid(x_vals.squeeze(), y_vals.squeeze(), indexing=\"ij\")\n",
        "xg_flat = Xg.reshape(-1,1)\n",
        "yg_flat = Yg.reshape(-1,1)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  u_pred_forward = model(xg_flat, yg_flat).cpu()\n",
        "  u_exact_all = u_exact_func(xg_flat.cpu(), yg_flat.cpu(), A=A_true)\n",
        "\n",
        "model_inv.eval()\n",
        "with torch.no_grad():\n",
        "  u_pred_inverse = model_inv(xg_flat, yg_flat).cpu()\n",
        "  u_exact_inv = u_exact_func(xg_flat.cpu(), yg_flat.cpu(), A=A_true)\n",
        "learned_alpha = alpha.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wexw4ySkeNzw"
      },
      "source": [
        "# **RESULTS VISUALISATION**\n",
        "\n",
        "## Compute Metrics and Show Sample Predictions\n",
        "\n",
        "In this step, we evaluate the performance of the forward and inverse PINN models:\n",
        "\n",
        "- Compute **Mean Squared Error (MSE)** between predicted and exact solutions.\n",
        "- Compute **Relative L2 Error**.\n",
        "- Compute approximate **accuracy** as a percentage.\n",
        "- Print the **learned parameter alpha** from the inverse problem.\n",
        "- Display a few **sample predictions** from both forward and inverse models alongside the exact values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xykEyUaeA4u"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(u_pred, u_exact):\n",
        "  diff = u_pred - u_exact\n",
        "  mse = torch.mean(diff**2).item()\n",
        "  rel_l2 = (torch.norm(diff) / torch.norm(u_exact)).item()\n",
        "  acc_percent = max(0.0, (1 - rel_l2) * 100)\n",
        "  return mse, rel_l2, acc_percent\n",
        "\n",
        "mse_fwd, rel_fwd, acc_fwd = compute_metrics(u_pred_forward, u_exact_all)\n",
        "mse_inv, rel_inv, acc_inv = compute_metrics(u_pred_inverse, u_exact_all)\n",
        "\n",
        "print(\"\\n--- Forward-trained model metrics ---\")\n",
        "print(\"MSE:\", mse_fwd)\n",
        "print(\"Relative L2 Error:\", rel_fwd)\n",
        "print(f\"Accuracy (approx): {acc_fwd:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Inverse-trained model metrics ---\")\n",
        "print(\"MSE:\", mse_inv)\n",
        "print(\"Relative L2 Error:\", rel_inv)\n",
        "print(f\"Accuracy (approx): {acc_inv:.2f}%\")\n",
        "print(\"Learned alpha (inverse):\", learned_alpha)\n",
        "print(\"True amplitude A_true:\", A_true)\n",
        "\n",
        "print(\"\\nSample predictions (forward model):\")\n",
        "for i in range(5):\n",
        "  print(f\"x={xg_flat[i].item():.3f}, y={yg_flat[i].item():.3f}, u_pred={u_pred_forward[i].item():.6f}, u_exact={u_exact_all[i].item():.6f}\")\n",
        "print(\"\\nSample predictions (inverse model):\")\n",
        "for i in range(5):\n",
        "  print(f\"x={xg_flat[i].item():.3f}, y={yg_flat[i].item():.3f}, u_pred={u_pred_inverse[i].item():.6f}, u_exact={u_exact_inv[i].item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izi6phOYeUFx"
      },
      "source": [
        "##Visualize Predictions and Errors\n",
        "\n",
        "In this step, we plot heatmaps to visually compare the forward and inverse PINN predictions against the exact solution:\n",
        "\n",
        "- First row: forward problem\n",
        "  - Predicted solution\n",
        "  - Exact solution\n",
        "  - Absolute error\n",
        "- Second row: inverse problem\n",
        "  - Predicted solution\n",
        "  - Exact solution\n",
        "  - Absolute error\n",
        "\n",
        "This helps to see where the model is accurate and where errors are larger.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg3QxTvpeEdQ"
      },
      "outputs": [],
      "source": [
        "u_pred_forward_np = u_pred_forward.numpy().reshape(n_test, n_test)\n",
        "u_exact_np = u_exact_all.numpy().reshape(n_test, n_test)\n",
        "u_pred_inverse_np = u_pred_inverse.numpy().reshape(n_test, n_test)\n",
        "\n",
        "fig, axs = plt.subplots(2,3, figsize=(18,10))\n",
        "\n",
        "axs[0,0].imshow(u_pred_forward_np, origin='lower', extent=[0,1,0,1])\n",
        "axs[0,0].set_title(\"Forward: Predicted\")\n",
        "axs[0,1].imshow(u_exact_np, origin='lower', extent=[0,1,0,1])\n",
        "axs[0,1].set_title(\"Exact\")\n",
        "axs[0,2].imshow(np.abs(u_pred_forward_np - u_exact_np), origin='lower', extent=[0,1,0,1])\n",
        "axs[0,2].set_title(\"Forward Absolute Error\")\n",
        "\n",
        "axs[1,0].imshow(u_pred_inverse_np, origin='lower', extent=[0,1,0,1])\n",
        "axs[1,0].set_title(\"Inverse: Predicted\")\n",
        "axs[1,1].imshow(u_exact_np, origin='lower', extent=[0,1,0,1])\n",
        "axs[1,1].set_title(\"Exact\")\n",
        "axs[1,2].imshow(np.abs(u_pred_inverse_np - u_exact_np), origin='lower', extent=[0,1,0,1])\n",
        "axs[1,2].set_title(\"Inverse Absolute Error\")\n",
        "\n",
        "for ax in axs.flat:\n",
        "  ax.set_xlabel(\"x\")\n",
        "  ax.set_ylabel(\"y\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}